import streamlit as st
import requests
import pandas as pd
from datetime import datetime, timedelta
import calendar
from ftplib import FTP
import io
import pytz
import logging
from bs4 import BeautifulSoup 
import re 
from typing import List, Dict, Any

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š (ãƒ‡ãƒãƒƒã‚°ç”¨)
logging.basicConfig(level=logging.INFO)

# æ—¥æœ¬ã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³
JST = pytz.timezone('Asia/Tokyo')

# --- å®šæ•°è¨­å®š ---

# --- å£²ä¸Šãƒ‡ãƒ¼ã‚¿è¨­å®š ---
SR_TIME_CHARGE_URL = "https://www.showroom-live.com/organizer/show_rank_time_charge_hist_invoice_format" 
SR_PREMIUM_LIVE_URL = "https://www.showroom-live.com/organizer/paid_live_hist_invoice_format" 
SR_ROOM_SALES_URL = "https://www.showroom-live.com/organizer/point_hist_with_mixed_rate" 

DATA_TYPES = {
    "time_charge": {
        "label": "ã‚¿ã‚¤ãƒ ãƒãƒ£ãƒ¼ã‚¸å£²ä¸Š",
        "url": SR_TIME_CHARGE_URL,
        "filename": "show_rank_time_charge_hist_invoice_format.csv",
        "type": "standard" 
    },
    "premium_live": {
        "label": "ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒ©ã‚¤ãƒ–å£²ä¸Š",
        "url": SR_PREMIUM_LIVE_URL,
        "filename": "paid_live_hist_invoice_format.csv",
        "type": "standard"
    },
    "room_sales": { 
        "label": "ãƒ«ãƒ¼ãƒ å£²ä¸Š",
        "url": SR_ROOM_SALES_URL,
        "filename": "point_hist_with_mixed_rate_csv_donwload_for_room.csv",
        "type": "room_sales"
    }
}

# --- KPIãƒ‡ãƒ¼ã‚¿è¨­å®š ---
SR_KPI_URL = "https://www.showroom-live.com/organizer/live_kpi"
KPI_MAX_PAGES = 5
# KPIãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå£²ä¸Šãƒ‡ãƒ¼ã‚¿ã¨ã¯ç•°ãªã‚‹çµ¶å¯¾ãƒ‘ã‚¹ã‚’å®šç¾©ï¼‰
KPI_FTP_BASE_PATH = "/mksoul-pro.com/showroom/csv/"


# --- è¨­å®šãƒ­ãƒ¼ãƒ‰ã¨èªè¨¼ (ä¿®æ­£) ---
try:
    # æ—¢å­˜ã®å…±é€šCookieï¼ˆå£²ä¸Š3ç‚¹ã‚»ãƒƒãƒˆç”¨ï¼‰
    AUTH_COOKIE_STRING = st.secrets["showroom"]["auth_cookie_string"]
    
    # ğŸš¨ ä¿®æ­£: KPIå°‚ç”¨Cookieã®èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã‚‹
    try:
        KPI_AUTH_COOKIE_STRING = st.secrets["showroom"]["kpi_auth_cookie_string"]
        st.info("KPIå°‚ç”¨ã®CookieãŒè¨­å®šã•ã‚Œã¾ã—ãŸã€‚KPIå‡¦ç†ã§ã¯ã“ã®Cookieã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    except KeyError:
        # KPIå°‚ç”¨CookieãŒsecretsã«ãªã„å ´åˆã¯ã€å…±é€šCookieã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨
        KPI_AUTH_COOKIE_STRING = AUTH_COOKIE_STRING
        st.warning("KPIå°‚ç”¨Cookie (`kpi_auth_cookie_string`) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…±é€šCookieã‚’KPIå‡¦ç†ã«ä½¿ç”¨ã—ã¾ã™ã€‚")


    FTP_CONFIG = {
        "host": st.secrets["ftp"]["host"],
        "user": st.secrets["ftp"]["user"],
        "password": st.secrets["ftp"]["password"],
        "target_base_path": st.secrets["ftp"]["target_base_path"] 
    }
    
    # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ãŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ï¼ˆæœ«å°¾ãŒ'/'ï¼‰ã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼
    if FTP_CONFIG["target_base_path"].endswith(".csv"):
        base_path = '/'.join(FTP_CONFIG["target_base_path"].split('/')[:-1]) + '/'
        FTP_CONFIG["target_base_path"] = base_path
    elif not FTP_CONFIG["target_base_path"].endswith('/'):
         FTP_CONFIG["target_base_path"] += '/'
    
except KeyError as e:
    AUTH_COOKIE_STRING = "DUMMY"
    KPI_AUTH_COOKIE_STRING = "DUMMY"
    FTP_CONFIG = None
    if str(e) == "'target_base_path'":
         st.error(f"ğŸš¨ FTPè¨­å®šãŒä¸å®Œå…¨ã§ã™ã€‚`target_path`ã§ã¯ãªã`target_base_path`ã‚’`.streamlit/secrets.toml`ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    else:
        st.error(f"ğŸš¨ èªè¨¼ã¾ãŸã¯FTPè¨­å®šãŒã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`.streamlit/secrets.toml`ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ä¸è¶³: {e}")
    st.stop()


# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---

def get_sales_months():
    """å£²ä¸Šãƒ‡ãƒ¼ã‚¿ç”¨: 2023å¹´10æœˆä»¥é™ã®æœˆãƒªã‚¹ãƒˆã‚’ 'YYYYå¹´MMæœˆåˆ†' å½¢å¼ã§ç”Ÿæˆã—ã€UNIXã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—ã™ã‚‹"""
    START_YEAR = 2023
    START_MONTH = 10 # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã¯10æœˆé–‹å§‹
    
    today = datetime.now(JST)
    months = []
    
    current_year = today.year
    current_month = today.month
    
    while True:
        if current_year < START_YEAR or (current_year == START_YEAR and current_month < START_MONTH):
            break

        month_str = f"{current_year}å¹´{current_month:02d}æœˆåˆ†"
        
        try:
            dt_naive = datetime(current_year, current_month, 1, 0, 0, 0)
            dt_obj_jst = JST.localize(dt_naive, is_dst=None)
            timestamp = int(dt_obj_jst.timestamp()) 
            
            months.append((month_str, timestamp)) 
        except Exception as e:
            logging.error(f"å£²ä¸Šæ—¥ä»˜è¨ˆç®—ã‚¨ãƒ©ãƒ¼ ({month_str}): {e}")
            
        if current_month == 1:
            current_month = 12
            current_year -= 1
        else:
            current_month -= 1
            
    return months


def get_kpi_months():
    """KPIãƒ‡ãƒ¼ã‚¿ç”¨: 2023å¹´9æœˆä»¥é™ã®æœˆãƒªã‚¹ãƒˆã‚’ 'YYYYå¹´MMæœˆåˆ†' å½¢å¼ã§ç”Ÿæˆã—ã€datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¨ˆç®—ã™ã‚‹"""
    START_YEAR = 2023
    START_MONTH = 9 # KPIãƒ‡ãƒ¼ã‚¿ã¯9æœˆé–‹å§‹
    
    today = datetime.now(JST)
    months = []
    
    current_year = today.year
    current_month = today.month
    
    while True:
        if current_year < START_YEAR or (current_year == START_YEAR and current_month < START_MONTH):
            break

        month_str = f"{current_year}å¹´{current_month:02d}æœˆåˆ†"
        
        try:
            dt_naive = datetime(current_year, current_month, 1, 0, 0, 0)
            
            months.append((month_str, dt_naive)) 
        except Exception as e:
            logging.error(f"KPIæ—¥ä»˜è¨ˆç®—ã‚¨ãƒ©ãƒ¼ ({month_str}): {e}")
            
        if current_month == 1:
            current_month = 12
            current_year -= 1
        else:
            current_month -= 1
            
    return months


def get_month_start_end_dates(month_dt: datetime) -> tuple[str, str, str]:
    """æœˆã®åˆæ—¥ ('YYYY-MM-01') ã¨æœ€çµ‚æ—¥ ('YYYY-MM-DD')ã€ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ ('YYYY-MM') ã‚’è¨ˆç®—ã™ã‚‹"""
    from_date_str = month_dt.strftime('%Y-%m-01')
    
    if month_dt.month == 12:
        next_month = month_dt.replace(year=month_dt.year + 1, month=1, day=1)
    else:
        next_month = month_dt.replace(month=month_dt.month + 1, day=1)
        
    last_day = next_month - timedelta(days=1)
    to_date_str = last_day.strftime('%Y-%m-%d')
    
    file_prefix = month_dt.strftime('%Y-%m')
    
    return from_date_str, to_date_str, file_prefix


def create_authenticated_session(cookie_string):
    """æ‰‹å‹•ã§å–å¾—ã—ãŸCookieæ–‡å­—åˆ—ã‹ã‚‰èªè¨¼æ¸ˆã¿Requestsã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹"""
    st.info("èªè¨¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™...")
    session = requests.Session()
    try:
        cookies_dict = {}
        for item in cookie_string.split(';'):
            item = item.strip()
            if '=' in item:
                name, value = item.split('=', 1)
                cookies_dict[name.strip()] = value.strip()
        cookies_dict['i18n_redirected'] = 'ja'
        session.cookies.update(cookies_dict)
        
        if not cookies_dict:
            st.error("ğŸš¨ æœ‰åŠ¹ãªèªè¨¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
            
        return session
    except Exception as e:
        st.error(f"èªè¨¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

# --- å£²ä¸Šãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ ---

def fetch_and_process_sales_data(timestamp, cookie_string, sr_url, data_type_key):
    """
    æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã«åŸºã¥ã„ã¦SHOWROOMã‹ã‚‰å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€BeautifulSoupã§æ•´å½¢ã™ã‚‹
    """
    st.info(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... URL: {sr_url}, ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {timestamp}")
    session = create_authenticated_session(cookie_string)
    if not session:
        return None
    
    try:
        # 1. ãƒ‡ãƒ¼ã‚¿å–å¾—
        url = f"{sr_url}?from={timestamp}" 
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Referer': sr_url
        }
        
        response = session.get(url, headers=headers, timeout=30)
        response.raise_for_status() 
        
        # 2. HTMLã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        soup = BeautifulSoup(response.text, 'html5lib') 
        table = soup.find('table', class_='table-type-02') 
        
        if not table:
            if "ãƒ­ã‚°ã‚¤ãƒ³" in response.text or "ä¼šå“¡ç™»éŒ²" in response.text:
                st.error("ğŸš¨ èªè¨¼åˆ‡ã‚Œã§ã™ã€‚CookieãŒå¤ã„ã‹ç„¡åŠ¹ã«ãªã£ã¦ã„ã¾ã™ã€‚")
                return None
            st.warning("HTMLã‹ã‚‰å£²ä¸Šãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ« (`table-type-02`) ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒšãƒ¼ã‚¸æ§‹é€ ãŒå¤‰æ›´ã•ã‚ŒãŸã‹ã€ãƒ‡ãƒ¼ã‚¿ãŒã¾ã ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            
        
        # 3. ãƒ‡ãƒ¼ã‚¿ã‚’BeautifulSoupã§æŠ½å‡º (ãƒ©ã‚¤ãƒãƒ¼å€‹åˆ¥ã®ãƒ‡ãƒ¼ã‚¿)
        table_data = []
        if table:
            rows = table.find_all('tr')
            
            for row in rows[1:]: 
                td_tags = row.find_all('td')
                
                if len(td_tags) >= 5:
                    amount_str = td_tags[3].text.strip().replace(',', '') 
                    account_id = td_tags[4].text.strip()
                    
                    if amount_str.isnumeric():
                         table_data.append({
                            'åˆ†é…é¡': amount_str, 
                            'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID': account_id
                        })
        
        # 4. DataFrameã«å¤‰æ›ã—ã€æ•´å½¢ (ãƒ­ã‚¸ãƒƒã‚¯ã®åˆ†å²)
        
        # 4-A. ãƒ«ãƒ¼ãƒ å£²ä¸Šã®ç‰¹æ®Šãƒ­ã‚¸ãƒƒã‚¯
        if data_type_key == "room_sales":
            
            total_amount_tag = soup.find('p', class_='fs-b4 bg-light-gray p-b3 mb-b2 link-light-green')
            total_amount_str = '0'
            if total_amount_tag:
                match = re.search(r'æ”¯æ‰•ã„é‡‘é¡ï¼ˆç¨æŠœï¼‰:\s*<span[^>]*>\s*([\d,]+)å††', str(total_amount_tag))
                
                if match:
                    total_amount_str = match.group(1).replace(',', '') 
                else:
                    st.warning("âš ï¸ HTMLã‹ã‚‰ã€Œæ”¯æ‰•ã„é‡‘é¡ï¼ˆç¨æŠœï¼‰ã€ã®å€¤ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åˆ†é…é¡ã‚’ã€Œ0ã€ã¨ã—ã¦å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")
                    
            header_data = [{
                'åˆ†é…é¡': total_amount_str,
                'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID': 'MKsoul'
            }]
            
            header_df = pd.DataFrame(header_data)
            
            if table_data:
                driver_df = pd.DataFrame(table_data)
                df_cleaned = pd.concat([header_df, driver_df], ignore_index=True)
                st.success(f"ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ ({len(driver_df)}ä»¶) ã®æŠ½å‡ºã¨åˆè¨ˆå€¤ ({total_amount_str}) ã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            else:
                df_cleaned = header_df
                st.warning(f"âš ï¸ ãƒ©ã‚¤ãƒãƒ¼å€‹åˆ¥ã®ãƒ‡ãƒ¼ã‚¿è¡Œã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åˆè¨ˆå€¤ ({total_amount_str}) ã¨ MKsoul ã®ã¿ã‚’å«ã‚€1è¡Œãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")


        # 4-B. ã‚¿ã‚¤ãƒ ãƒãƒ£ãƒ¼ã‚¸/ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒ©ã‚¤ãƒ–ã®æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ (0ä»¶æ™‚ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ)
        else: # time_charge or premium_live
            if not table_data:
                st.warning("âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿è¡Œã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åˆ†é…é¡=0ã€ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID=dummyã‚’å«ã‚€1è¡Œãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™ã€‚")
                
                df_cleaned = pd.DataFrame([{
                    'åˆ†é…é¡': '0',       
                    'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID': 'dummy' 
                }])
                
            else:
                st.success(f"ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ ({len(table_data)}ä»¶) ã®æŠ½å‡ºãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
                df_cleaned = pd.DataFrame(table_data)

        # 5. ç‰¹æ®ŠãªCSVå½¢å¼ã®ä½œæˆï¼ˆå…±é€šãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        
        now_jst = datetime.now(JST)
        update_time_str = now_jst.strftime('%Y/%m/%d %H:%M')
        
        final_df = pd.DataFrame({
            'åˆ†é…é¡': df_cleaned['åˆ†é…é¡'],
            'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID': df_cleaned['ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID'],
            'æ›´æ–°æ—¥æ™‚': '' 
        })
        
        if not final_df.empty:
            final_df.loc[0, 'æ›´æ–°æ—¥æ™‚'] = update_time_str
        
        csv_buffer = io.StringIO()
        final_df.to_csv(csv_buffer, index=False, header=False, encoding='utf-8')
        
        st.success("ãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        st.code(csv_buffer.getvalue(), language='text') 
        
        return csv_buffer
        
    except requests.exceptions.HTTPError as e:
        st.error(f"HTTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e.response.status_code}. èªè¨¼CookieãŒç„¡åŠ¹ã«ãªã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return None
    except Exception as e:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logging.error("ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ•´å½¢ã‚¨ãƒ©ãƒ¼", exc_info=True)
        return None


# --- KPIãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ (æ–°è¦) ---

def fetch_and_process_kpi_data(month_dt: datetime, cookie_string: str) -> pd.DataFrame or None:
    """
    æŒ‡å®šã•ã‚ŒãŸæœˆï¼ˆmonth_dtï¼‰ã«åŸºã¥ã„ã¦KPIãƒ‡ãƒ¼ã‚¿ã‚’æœ€å¤§5ãƒšãƒ¼ã‚¸å–å¾—ã—ã€æ•´å½¢ã‚’è¡Œã†
    """
    
    from_date_str, to_date_str, file_prefix = get_month_start_end_dates(month_dt)
    st.info(f"KPIãƒ‡ãƒ¼ã‚¿å–å¾—æœŸé–“: {from_date_str} ã‹ã‚‰ {to_date_str} ã¾ã§ (æœ€å¤§ {KPI_MAX_PAGES} ãƒšãƒ¼ã‚¸)")
    # process_kpi_toolã‹ã‚‰æ¸¡ã•ã‚ŒãŸå°‚ç”¨(ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)ã®cookie_stringã‚’ä½¿ç”¨
    session = create_authenticated_session(cookie_string) 
    if not session:
        return None
        
    # å‰å›è©¦è¡Œã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å‡¦ç†ã¯å‰Šé™¤ã—ã€æ–°Cookieã§ã®èªè¨¼ã«é›†ä¸­ã—ã¾ã™ã€‚
    
    all_kpi_data: List[Dict[str, Any]] = []
    
    CSV_HEADERS = [
        "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID", "ãƒ«ãƒ¼ãƒ ID", "é…ä¿¡æ—¥æ™‚", "é…ä¿¡æ™‚é–“(åˆ†)", "é€£ç¶šé…ä¿¡æ—¥æ•°", "ãƒ«ãƒ¼ãƒ å", 
        "åˆè¨ˆè¦–è´æ•°", "è¦–è´ä¼šå“¡æ•°", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ä¼šå“¡æ•°", "SPã‚®ãƒ•ãƒˆä½¿ç”¨ä¼šå“¡ç‡", "åˆãƒ«ãƒ¼ãƒ æ¥è¨ªè€…æ•°", 
        "åˆSRæ¥è¨ªè€…æ•°", "çŸ­æ™‚é–“æ»åœ¨è€…æ•°", "ãƒ«ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¢—æ¸›æ•°", 
        "Postäººæ•°", "ç²å¾—æ”¯æ´point", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "åˆã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", 
        "ã‚®ãƒ•ãƒˆæ•°", "ã‚®ãƒ•ãƒˆäººæ•°", "åˆã‚®ãƒ•ãƒˆäººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°æ•°", 
        "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°äººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGç·é¡", "2023å¹´9æœˆä»¥å‰ã®ãŠã¾ã‘åˆ†(ç„¡å„ŸSG RSå¤–)"
    ]
    
    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ—
    for page_num in range(1, KPI_MAX_PAGES + 1):
        try:
            url = (f"{SR_KPI_URL}?page={page_num}&room_id=&from_date={from_date_str}&to_date={to_date_str}")
            st.info(f"â¡ï¸ ãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ä¸­...")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
                'Referer': SR_KPI_URL
            }
            
            response = session.get(url, headers=headers, timeout=30)
            response.raise_for_status() 
            
            soup = BeautifulSoup(response.text, 'html5lib') 
            table = soup.find('table', class_='table-type-02') 
            
            if not table:
                if "ãƒ­ã‚°ã‚¤ãƒ³" in response.text:
                    st.error("ğŸš¨ èªè¨¼åˆ‡ã‚Œã§ã™ã€‚CookieãŒå¤ã„ã‹ç„¡åŠ¹ã«ãªã£ã¦ã„ã¾ã™ã€‚")
                    return None
                st.warning(f"ãƒšãƒ¼ã‚¸ {page_num}: ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ãŒçµ‚äº†ã—ãŸã‹ã€ãƒšãƒ¼ã‚¸æ§‹é€ ãŒå¤‰æ›´ã•ã‚Œã¦ã„ã¾ã™ã€‚")
                break 

            rows = table.find_all('tr')
            
            page_data = []
            if len(rows) <= 1: 
                st.info(f"ãƒšãƒ¼ã‚¸ {page_num}: æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿è¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å–å¾—ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                break 

            for row in rows[1:]: 
                td_tags = row.find_all('td')
                
                if len(td_tags) != 28:
                    continue 

                row_data: Dict[str, Any] = {}
                
                # é…ä¿¡æ—¥æ™‚ã¨é…ä¿¡æ™‚é–“(åˆ†)ã®ç‰¹æ®Šå‡¦ç† (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹2)
                time_data = td_tags[2].text.strip()
                match_time = re.search(r'(\d{4}-\d{2}-\d{2}\s\d{2}:\d{2}:\d{2}).*?\((\d+)m(\d+)s\)', time_data)

                if match_time:
                    start_datetime_str = match_time.group(1).replace('-', '/')
                    minutes = int(match_time.group(2))
                    seconds = int(match_time.group(3))
                    duration_min = minutes + 1 if seconds >= 30 else minutes
                else:
                    start_datetime_str = ''
                    duration_min = 0

                row_data["é…ä¿¡æ—¥æ™‚"] = start_datetime_str
                row_data["é…ä¿¡æ™‚é–“(åˆ†)"] = str(duration_min)
                
                # ãã®ä»–ã®åˆ—ã®æŠ½å‡º
                for i, header in enumerate(CSV_HEADERS):
                    if i == 2 or header == "é…ä¿¡æ™‚é–“(åˆ†)":
                        continue
                        
                    content = td_tags[i].text.strip()
                    
                    if header in ["åˆè¨ˆè¦–è´æ•°", "è¦–è´ä¼šå“¡æ•°", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ä¼šå“¡æ•°", "ç²å¾—æ”¯æ´point", "ã‚³ãƒ¡ãƒ³ãƒˆæ•°", "ã‚®ãƒ•ãƒˆæ•°", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°", "ãƒ«ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«", "åˆãƒ«ãƒ¼ãƒ æ¥è¨ªè€…æ•°", "åˆSRæ¥è¨ªè€…æ•°", "çŸ­æ™‚é–“æ»åœ¨è€…æ•°", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼å¢—æ¸›æ•°", "Postäººæ•°", "ã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "åˆã‚³ãƒ¡ãƒ³ãƒˆäººæ•°", "ã‚®ãƒ•ãƒˆäººæ•°", "åˆã‚®ãƒ•ãƒˆäººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°æ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGã®ã‚®ãƒ•ãƒ†ã‚£ãƒ³ã‚°äººæ•°", "æœŸé™ã‚ã‚Š/æœŸé™ãªã—SGç·é¡"]:
                        content = content.replace(',', '')
                    elif header == "SPã‚®ãƒ•ãƒˆä½¿ç”¨ä¼šå“¡ç‡":
                        content = content.replace('%', '')
                    elif header == "ãƒ«ãƒ¼ãƒ å":
                        div_tag = td_tags[i].find('div')
                        if div_tag:
                            content = div_tag.text.strip()

                    if i in [0, 1]:
                        a_tag = td_tags[i].find('a')
                        if a_tag:
                            content = a_tag.text.strip()
                        
                    row_data[header] = content
                    
                page_data.append(row_data)

            if page_data:
                st.success(f"ãƒšãƒ¼ã‚¸ {page_num}: {len(page_data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
                all_kpi_data.extend(page_data)
            
            if len(page_data) < 1000:
                 st.info(f"ãƒšãƒ¼ã‚¸ {page_num} ã®å–å¾—ä»¶æ•°ãŒ1000ä»¶æœªæº€ã ã£ãŸãŸã‚ã€ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                 break

        except requests.exceptions.HTTPError as e:
            st.error(f"ãƒšãƒ¼ã‚¸ {page_num} ã§HTTPã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e.response.status_code}. èªè¨¼CookieãŒç„¡åŠ¹ã«ãªã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            return None
        except Exception as e:
            st.error(f"ãƒšãƒ¼ã‚¸ {page_num} ã§äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            logging.error(f"KPIãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ (ãƒšãƒ¼ã‚¸ {page_num})", exc_info=True)
            return None
    
    if not all_kpi_data:
        st.warning("âš ï¸ æœŸé–“å†…ã®KPIãƒ‡ãƒ¼ã‚¿ãŒå…¨ãæŠ½å‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return pd.DataFrame(columns=CSV_HEADERS)
    
    df = pd.DataFrame(all_kpi_data, columns=CSV_HEADERS)
    
    # é‡è¤‡é™¤å¤– (é‡è¤‡é™¤å¤–ã‚­ãƒ¼ã¯ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID, ãƒ«ãƒ¼ãƒ ID, é…ä¿¡æ—¥æ™‚, é…ä¿¡æ™‚é–“(åˆ†))
    dedup_keys = ["ã‚¢ã‚«ã‚¦ãƒ³ãƒˆID", "ãƒ«ãƒ¼ãƒ ID", "é…ä¿¡æ—¥æ™‚", "é…ä¿¡æ™‚é–“(åˆ†)"]
    original_count = len(df)
    df_cleaned = df.drop_duplicates(subset=dedup_keys, keep='first')
    dedup_count = len(df_cleaned)

    if original_count > dedup_count:
        st.success(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ•´å½¢ãŒå®Œäº†ã—ã¾ã—ãŸã€‚é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ {original_count - dedup_count} ä»¶é™¤å¤–ã—ã¾ã—ãŸã€‚æœ€çµ‚ä»¶æ•°: {dedup_count} ä»¶ã€‚")
    else:
        st.success(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ•´å½¢ãŒå®Œäº†ã—ã¾ã—ãŸã€‚æœ€çµ‚ä»¶æ•°: {dedup_count} ä»¶ã€‚")

    return df_cleaned


# --- FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–¢æ•° ---
def upload_file_ftp(csv_buffer, ftp_config, full_target_path):
    # ... (å¤‰æ›´ãªã—) ...
    """
    FTPã‚µãƒ¼ãƒãƒ¼ã«æ•´å½¢æ¸ˆã¿CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ 
    """
    st.info(f"FTPã‚µãƒ¼ãƒãƒ¼ ({ftp_config['host']}) ã«æ¥ç¶šã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™... (ãƒ‘ã‚¹: {full_target_path})")
    
    try:
        csv_buffer.seek(0)
        # FTPæ¥ç¶š
        with FTP(ftp_config['host'], ftp_config['user'], ftp_config['password']) as ftp:
            csv_bytes = csv_buffer.getvalue().encode('utf-8')
            
            ftp.storbinary(f'STOR {full_target_path}', io.BytesIO(csv_bytes))
            
            st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            st.markdown(f"**ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å…ˆ:** `{ftp_config['host']}:{full_target_path}`")
            
    except Exception as e:
        st.error(f"FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è¨­å®šï¼ˆãƒ›ã‚¹ãƒˆåã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã€ãƒ‘ã‚¹ï¼‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {e}")
        logging.error("FTPã‚¨ãƒ©ãƒ¼", exc_info=True)
        return False
        
    return True


# --- ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•° ---

def process_sales_tool(data_type_key, selected_timestamp, auth_cookie_string, ftp_config):
    # ... (å¤‰æ›´ãªã—) ...
    """
    å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ï¼ˆã‚¿ã‚¤ãƒ ãƒãƒ£ãƒ¼ã‚¸ã€ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒ©ã‚¤ãƒ–ã€ã¾ãŸã¯ãƒ«ãƒ¼ãƒ å£²ä¸Šï¼‰ã®å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
    """
    data_info = DATA_TYPES[data_type_key]
    data_label = data_info["label"]
    sr_url = data_info["url"]
    filename = data_info["filename"]
    
    full_target_path = ftp_config["target_base_path"] + filename
    
    st.subheader(f"ğŸ”„ **{data_label}** ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    
    csv_buffer = fetch_and_process_sales_data(selected_timestamp, auth_cookie_string, sr_url, data_type_key)
    
    if csv_buffer:
        if ftp_config:
            upload_file_ftp(csv_buffer, ftp_config, full_target_path)
        else:
            st.error("FTPè¨­å®šãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")
    else:
        st.error(f"{data_label}ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ•´å½¢ã«å¤±æ•—ã—ãŸãŸã‚ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")
        
    st.markdown("---")

def process_kpi_tool(selected_month_dt_list: List[datetime], auth_cookie_string: str, ftp_config: Dict[str, str]):
    # ... (å¼•æ•°ã®auth_cookie_stringã«ã¯KPI_AUTH_COOKIE_STRINGãŒæ¸¡ã•ã‚Œã‚‹) ...
    """
    KPIãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ•´å½¢ãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®å‡¦ç†ã‚’è¤‡æ•°æœˆã«å¯¾ã—ã¦å®Ÿè¡Œã™ã‚‹
    """
    
    if not selected_month_dt_list:
        st.warning("âš ï¸ å‡¦ç†å¯¾è±¡ã®æœˆãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
        
    selected_month_dt_list.sort() 
    
    st.subheader(f"ğŸ“Š **é…ä¿¡KPIãƒ‡ãƒ¼ã‚¿** ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ ({len(selected_month_dt_list)}ãƒ¶æœˆåˆ†)")
    
    for month_dt in selected_month_dt_list:
        month_label = month_dt.strftime('%Yå¹´%mæœˆåˆ†')
        st.info(f"--- {month_label} ã®å‡¦ç† ---")
        
        from_date_str, to_date_str, file_prefix = get_month_start_end_dates(month_dt)
        target_filename = f"{file_prefix}_all_all.csv"
        
        full_target_path = KPI_FTP_BASE_PATH + target_filename
        
        # 1. ãƒ‡ãƒ¼ã‚¿å–å¾—ã¨æ•´å½¢ï¼ˆDataFrameã‚’è¿”ã™ï¼‰
        df_cleaned = fetch_and_process_kpi_data(month_dt, auth_cookie_string) # æ¸¡ã•ã‚ŒãŸCookieã‚’ä½¿ç”¨
        
        if df_cleaned is not None:
            
            # 2. CSVãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ãƒ¡ãƒ¢ãƒªã«æ›¸ãå‡ºã™
            csv_buffer = io.StringIO()
            df_cleaned.to_csv(csv_buffer, index=False, header=True, encoding='utf-8')
            
            st.success(f"ã€{month_label}ã€‘ã®ãƒ‡ãƒ¼ã‚¿æ•´å½¢ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ä»¶æ•°: {len(df_cleaned)}ä»¶ã€‚")
            st.code(csv_buffer.getvalue()[:2000] + "\n...", language='csv') 
            
            # 3. FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            if ftp_config:
                upload_file_ftp(csv_buffer, ftp_config, full_target_path)
            else:
                st.error("FTPè¨­å®šãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")
        else:
            st.error(f"ã€{month_label}ã€‘ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ•´å½¢ã«å¤±æ•—ã—ãŸãŸã‚ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚")
            
        st.markdown("---")


# --- Streamlit UI (ä¿®æ­£) ---

def main():
    st.set_page_config(page_title="SHOWROOMãƒ‡ãƒ¼ã‚¿ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ„ãƒ¼ãƒ«", layout="wide")
    st.title("SHOWROOMãƒ‡ãƒ¼ã‚¿ è‡ªå‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ„ãƒ¼ãƒ« (å£²ä¸Š3ç¨® & é…ä¿¡KPI)")
    st.markdown("---")

    
    # 1. å£²ä¸Šãƒ‡ãƒ¼ã‚¿ç”¨ã®æœˆé¸æŠ (2023å¹´10æœˆä»¥é™ã€ã‚·ãƒ³ã‚°ãƒ«ã‚»ãƒ¬ã‚¯ãƒˆ)
    sales_month_options = get_sales_months()
    sales_month_labels = [label for label, _ in sales_month_options]
    
    st.header("1. å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆ3ç¨®ï¼‰å‡¦ç†å¯¾è±¡æœˆé¸æŠ")
    
    selected_sales_label = st.selectbox(
        "å£²ä¸Šãƒ‡ãƒ¼ã‚¿ï¼ˆ2023å¹´10æœˆä»¥é™ï¼‰ã®å‡¦ç†å¯¾è±¡æœˆã‚’é¸æŠã—ã¦ãã ã•ã„:",
        options=sales_month_labels,
        index=0 
    )
    
    selected_sales_timestamp = next((ts for label, ts in sales_month_options if label == selected_sales_label), None)

    if selected_sales_timestamp is None:
        st.warning("æœ‰åŠ¹ãªå£²ä¸Šå‡¦ç†æœˆãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
        
    st.info(f"é¸æŠã•ã‚ŒãŸå£²ä¸Šå‡¦ç†æœˆ: **{selected_sales_label}**")
    
    # --- å£²ä¸Šãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬å‡¦ç†ãƒœã‚¿ãƒ³ ---
    if st.button("ğŸš€ å£²ä¸Šãƒ‡ãƒ¼ã‚¿3ç¨®ï¼ˆã‚¿ã‚¤ãƒ ãƒãƒ£ãƒ¼ã‚¸/ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ãƒ©ã‚¤ãƒ–/ãƒ«ãƒ¼ãƒ å£²ä¸Šï¼‰ã‚’å–å¾—ãƒ»FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ", type="primary"):
        with st.spinner(f"å£²ä¸Šãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­: {selected_sales_label}ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã„ã¾ã™..."):
            
            # å…±é€šCookie (AUTH_COOKIE_STRING) ã‚’ä½¿ç”¨
            process_sales_tool("time_charge", selected_sales_timestamp, AUTH_COOKIE_STRING, FTP_CONFIG)
            process_sales_tool("premium_live", selected_sales_timestamp, AUTH_COOKIE_STRING, FTP_CONFIG)
            process_sales_tool("room_sales", selected_sales_timestamp, AUTH_COOKIE_STRING, FTP_CONFIG)

        st.balloons()
        st.success("ğŸ‰ **å£²ä¸Šãƒ‡ãƒ¼ã‚¿3ç¨®ã®å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼**")
        
    st.markdown("---")

    # 2. KPIãƒ‡ãƒ¼ã‚¿ç”¨ã®æœˆé¸æŠ (2023å¹´9æœˆä»¥é™ã€ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆ)
    kpi_month_options = get_kpi_months()
    kpi_month_labels = [label for label, _ in kpi_month_options]
    
    st.header("2. é…ä¿¡KPIãƒ‡ãƒ¼ã‚¿å‡¦ç†å¯¾è±¡æœˆé¸æŠ")
    
    default_selection = kpi_month_labels[0] if kpi_month_labels else None
    
    selected_kpi_labels = st.multiselect(
        "é…ä¿¡KPIãƒ‡ãƒ¼ã‚¿ï¼ˆ2023å¹´9æœˆä»¥é™ï¼‰ã®å‡¦ç†å¯¾è±¡æœˆã‚’è¤‡æ•°é¸æŠã—ã¦ãã ã•ã„:",
        options=kpi_month_labels,
        default=[default_selection] if default_selection else []
    )
    
    selected_kpi_dt_list = [dt for label, dt in kpi_month_options if label in selected_kpi_labels]

    if selected_kpi_labels:
        st.info(f"é¸æŠã•ã‚ŒãŸKPIå‡¦ç†æœˆ: **{', '.join(selected_kpi_labels)}**")
    
    # --- KPIãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒœã‚¿ãƒ³ ---
    if st.button("ğŸ“Š é…ä¿¡KPIãƒ‡ãƒ¼ã‚¿ ã‚’å–å¾—ãƒ»FTPã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ", type="secondary"):
        with st.spinner(f"KPIãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­: é¸æŠã•ã‚ŒãŸæœˆ ({len(selected_kpi_dt_list)}ãƒ¶æœˆ) ã®KPIãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã„ã¾ã™..."):
            
            # ğŸš¨ ä¿®æ­£: KPIå°‚ç”¨Cookie (KPI_AUTH_COOKIE_STRING) ã‚’ä½¿ç”¨
            process_kpi_tool(selected_kpi_dt_list, KPI_AUTH_COOKIE_STRING, FTP_CONFIG)

        st.balloons()
        st.success("ğŸ‰ **é…ä¿¡KPIãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼**")


if __name__ == "__main__":
    main()